{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6111f9b8",
   "metadata": {},
   "source": [
    "Lets code the boiler plate code we will use for every experiment below i.e. the imports, data preprocessing etc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a077b94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.datasets import imdb\n",
    "\n",
    "(train_features, train_labels), (test_features,test_labels) = imdb.load_data(num_words=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "252a8f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def multi_hot_encode(sequences,num_classes=10000):\n",
    "    results = np.zeros((len(sequences),num_classes))\n",
    "    for i,sequence in enumerate(sequences):\n",
    "        results[i][sequence] = 1.0\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fd20cd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain = multi_hot_encode(train_features)\n",
    "xtest = multi_hot_encode(test_features)\n",
    "\n",
    "ytrain = train_labels.astype('float32')\n",
    "ytest = test_labels.astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff12da77",
   "metadata": {},
   "source": [
    "First experiment is training the model with 1 or 3 layers and see the performence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a945b250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8087 - loss: 0.4940\n",
      "Epoch 2/4\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9022 - loss: 0.2924\n",
      "Epoch 3/4\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9238 - loss: 0.2265\n",
      "Epoch 4/4\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9374 - loss: 0.1895\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 711us/step - accuracy: 0.8862 - loss: 0.2837\n",
      "[0.2837281823158264, 0.8861600160598755]\n",
      "Loss : 0.284, Accuracy: 0.89\n"
     ]
    }
   ],
   "source": [
    "from keras import layers\n",
    "#experimenting with 1 layer\n",
    "model_1 = keras.Sequential([\n",
    "    layers.Dense(16,activation='relu'),\n",
    "    layers.Dense(1,activation='sigmoid'),\n",
    "])\n",
    "\n",
    "model_1.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "history = model_1.fit(xtrain,ytrain,epochs=4,batch_size=512)\n",
    "results = model_1.evaluate(xtest,ytest)\n",
    "print(results)\n",
    "print(f'Loss : {results[0]:.3f}, Accuracy: {results[1]:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46bc4f2",
   "metadata": {},
   "source": [
    "As you can see there is no significant change in accuracy or loss on the test set evaluation, Experiment 1 done\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e38655",
   "metadata": {},
   "source": [
    "Lets move on to the second one lets now use 3 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f77158cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8003 - loss: 0.5057\n",
      "Epoch 2/4\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9118 - loss: 0.2389\n",
      "Epoch 3/4\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9397 - loss: 0.1680\n",
      "Epoch 4/4\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9560 - loss: 0.1270\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 717us/step - accuracy: 0.8758 - loss: 0.3436\n",
      "[0.34359386563301086, 0.8758000135421753]\n",
      "Loss : 0.344, Accuracy: 0.88\n"
     ]
    }
   ],
   "source": [
    "from keras import layers\n",
    "#experimenting with 1 layer\n",
    "model_2 = keras.Sequential([\n",
    "    layers.Dense(16,activation='relu'),\n",
    "    layers.Dense(16,activation='relu'),\n",
    "    layers.Dense(16,activation='relu'),\n",
    "    layers.Dense(1,activation='sigmoid'),\n",
    "])\n",
    "\n",
    "model_2.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "history = model_2.fit(xtrain,ytrain,epochs=4,batch_size=512)\n",
    "results = model_2.evaluate(xtest,ytest)\n",
    "print(results)\n",
    "print(f'Loss : {results[0]:.3f}, Accuracy: {results[1]:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7921a8",
   "metadata": {},
   "source": [
    "to have more layers actually hurt the model the training accuracy was 96 while the testing was just 87 a huge difference "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ba34a8",
   "metadata": {},
   "source": [
    "lets use tanh activation and see the difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a787e72d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8225 - loss: 0.4279\n",
      "Epoch 2/4\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9205 - loss: 0.2179\n",
      "Epoch 3/4\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9455 - loss: 0.1590\n",
      "Epoch 4/4\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9602 - loss: 0.1241\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 704us/step - accuracy: 0.8724 - loss: 0.3399\n",
      "[0.3398858308792114, 0.872439980506897]\n",
      "Loss : 0.340, Accuracy: 0.87\n"
     ]
    }
   ],
   "source": [
    "from keras import layers\n",
    "#experimenting with 1 layer\n",
    "model_3 = keras.Sequential([\n",
    "    layers.Dense(16,activation='tanh'),\n",
    "    layers.Dense(16,activation='tanh'),\n",
    "    layers.Dense(1,activation='sigmoid'),\n",
    "])\n",
    "\n",
    "model_3.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "history = model_3.fit(xtrain,ytrain,epochs=4,batch_size=512)\n",
    "results = model_3.evaluate(xtest,ytest)\n",
    "print(results)\n",
    "print(f'Loss : {results[0]:.3f}, Accuracy: {results[1]:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929b5d20",
   "metadata": {},
   "source": [
    "the model performed the same as using the relu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eccfad2c",
   "metadata": {},
   "source": [
    "from my experiments i deduced for this particular dataset it's best to use a single layer, lets tweak the neurons for this layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ff45b1",
   "metadata": {},
   "source": [
    "lets use a list of neurons and see whats the testing loss on each of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c844bf17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(n_neurons):\n",
    "    from keras import layers\n",
    "    #experimenting with 1 layer\n",
    "    model = keras.Sequential([\n",
    "        layers.Dense(n_neurons,activation='relu'),\n",
    "        layers.Dense(1,activation='sigmoid'),\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy'],\n",
    "    )\n",
    "\n",
    "    history = model.fit(xtrain,ytrain,epochs=4,batch_size=512)\n",
    "    results = model.evaluate(xtest,ytest)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e4c22d9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8043 - loss: 0.5101\n",
      "Epoch 2/4\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8979 - loss: 0.3140\n",
      "Epoch 3/4\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9187 - loss: 0.2484\n",
      "Epoch 4/4\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9298 - loss: 0.2117\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 702us/step - accuracy: 0.8858 - loss: 0.2886\n",
      "for 8 Neurons, Loss : 0.289, Accuracy: 0.89\n",
      "Epoch 1/4\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8107 - loss: 0.4743\n",
      "Epoch 2/4\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9070 - loss: 0.2769\n",
      "Epoch 3/4\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9255 - loss: 0.2183\n",
      "Epoch 4/4\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9382 - loss: 0.1832\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 849us/step - accuracy: 0.8867 - loss: 0.2840\n",
      "for 16 Neurons, Loss : 0.284, Accuracy: 0.89\n",
      "Epoch 1/4\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8306 - loss: 0.4202\n",
      "Epoch 2/4\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9191 - loss: 0.2303\n",
      "Epoch 3/4\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9388 - loss: 0.1807\n",
      "Epoch 4/4\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9504 - loss: 0.1504\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 866us/step - accuracy: 0.8819 - loss: 0.2985\n",
      "for 32 Neurons, Loss : 0.298, Accuracy: 0.88\n",
      "Epoch 1/4\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8424 - loss: 0.3928\n",
      "Epoch 2/4\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9239 - loss: 0.2124\n",
      "Epoch 3/4\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9445 - loss: 0.1635\n",
      "Epoch 4/4\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9547 - loss: 0.1341\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 911us/step - accuracy: 0.8757 - loss: 0.3204\n",
      "for 64 Neurons, Loss : 0.320, Accuracy: 0.88\n"
     ]
    }
   ],
   "source": [
    "neurons = [8,16,32,64]\n",
    "for n in neurons:\n",
    "    results = get_model(n)\n",
    "    print(f'for {n} Neurons, Loss : {results[0]:.3f}, Accuracy: {results[1]:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6575ea89",
   "metadata": {},
   "source": [
    "it seems to me that 16 neurons is the best number, so from my experiments I understood that its better to use 1 layer with relu activation and 16 neurons"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
